} else {
leaf_success_C[[leaf]] = -1
}
}
leaf_success[[max(leaves)+1]]  = NULL
leaf_success_C[[max(leaves)+1]]  = NULL
#leaf_failures[[leaf]] contains the nodes from which leaf is right-descended
leaf_failures = NULL
leaf_failures[[max(leaves)+1]] = rep(0,5)
leaf_failures_C = NULL
leaf_failures_C[[max(leaves)+1]] = rep(0,5)
for (leaf in leaves){
node_list=NULL
node_list = c(leaf,ancestors[[leaf]])
failures = NULL
for (node in ancestors[[leaf]]){
if ((descendants[[node]][2] %in% node_list)==TRUE){
failures = c(failures,node)
}
}
if (is.null(failures)==FALSE){
leaf_failures[[leaf]] = failures
leaf_failures_C[[leaf]] = failures - 1
} else {
leaf_failures_C[[leaf]] = -1
}
}
leaf_failures[[max(leaves)+1]] = NULL
leaf_failures_C[[max(leaves)+1]] = NULL
#come up with a mapping from internal nodes to 1:p
#node_map[internal_nodes] in {1,2,dots,p}
p = length(internal_nodes)
node_map = rep(0,A)
for(x in 1:p){
node_map[internal_nodes[x]] = x
}
#find number of samples
D = ncol(test_dtm)
#Initialize an extra count matrix
extra_dtm = matrix(0,nrow=V,ncol=D)
#if there are less than 3 sequencing reads, it stays in the test set
#This doesn't matter because the count is so low
#If it is more than three, we split about in half
#This is a janky way to handle even and odd counts
for(d in 1:D){
for(v in 1:V){
total = test_dtm[v,d]
if( total > 3){
extra_dtm[v,d] = sample( (round(total/2) - 1):(round(total/2) + 1),1)
test_dtm[v,d] = test_dtm[v,d]-extra_dtm[v,d]
}
}
}
#convert test_dtm count data to list of vectors of tokens
docs = NULL
docs[[D+1]] = rep(0,10000)
docs_C = NULL
for (d in 1:D){ #for each document
doc = NULL
for(v in 1:V){
doc = c(doc,rep(v,test_dtm[v,d]))
}
doc = doc[sample(1:length(doc),replace = FALSE)]
docs[[d]] = doc
docs_C[[d]] = docs[[d]] - 1
}
docs[[D+1]] = NULL
#1 Initialize subocmmunity assignments to sequencing in each sample
ta = lapply(docs, function(x) rep(0, length(x))) # initialize topic assignment list
ta_C = lapply(docs, function(x) rep(0, length(x)))
#2 Generate word-topic (ASV-subcommunity) count matrix.
wt = matrix(0, K, V) # wt[k,v] is the count of word v assigned to topic k
#3 Initialize document-topic (sample-subcommunity) count matrix
dt = matrix(0, length(docs), K) #dt[d,k] is count of topic k in document d
#4 Initialize counts of words-by-document-by topic
# (ASVs-by-sample-by-subcommunity)
wc_dwt = array(0,dim=c(D,V,K))
#wc_dwt[d,w,k] is count of word w assigned to topic k in document d
#5 Initialie counts at each node by document and topic (sample and subcommunity)
nc_dnt =  array(0,dim=c(D,max(tree.edge),K)) #node-count-by-document-by-topic
for(d in 1:length(docs)){ # for each document
for(w in 1:length(docs[[d]])){ # for each token in document d
ta[[d]][w] = sample(1:K, 1) # randomly assign topic to token
ta_C[[d]][w] = ta[[d]][w]-1
ti = ta[[d]][w] # topic index
wi = docs[[d]][w] # word associated with token w
wt[ti,wi] = wt[ti,wi]+1 # update word-topic count matrix
}
#Use subcommunity-assignment vector to:
#   find sample-subcommunity (dt) counts
#   find ASVs-by-sample-by-subcommunity (wc_dwt) counts
for(k in 1:K){ # for each topic k
dt[d,k] = sum(ta[[d]]==k) # count tokens in document d assigned to topic t
ta.temp = docs[[d]][which(ta[[d]]==k)]
for(w in 1:length(ta.temp)){ # for each word
ti = k # topic index
wi = ta.temp[w] # wordID for token w
wc_dwt[d,wi,ti] = wc_dwt[d,wi,ti]+1 # update word-topic count matrix
}
nc_dnt[d,1:max(leaves),k]=wc_dwt[d,,k] #word-count,doc d,top k on the leaves
#recursively go up the tree to find counts on all nodes
for(i in length(layers):1){
for (j in 1:length(layers[[i]])){
if (length(descendants[[layers[[i]][j]]])>0){
nc_dnt[d,layers[[i]][j],k] = sum(nc_dnt[d,descendants[[layers[[i]][j]]],k])
}
}
}
}
}
#Initialize kappa
#kappa_pdk[p,d,k] is kappa for node p in sample d and subcommunity k
kappa_pdk = array(0,dim=c(p,D,K))
for(k in 1:K){
for(a in 1:p){
for(d in 1:D){
kappa_pdk[a,d,k] = nc_dnt[d,descendants[[internal_nodes[a]]][1],k] - nc_dnt[d,internal_nodes[a],k]/2
}
}
}
#initialize phi according to a Dir(1) distribution
#phi_dk[d,k] is the proportion of subcommunity k in sample d
phi_dk = matrix(0,nrow=D,ncol=K)
for(d in 1:D){
for(k in 1:K){
phi_dk[d,] = stats::rgamma(K,1,1)
}
phi_dk[d,] = phi_dk[d,]/sum(phi_dk[d,])
}
#initialize psi according to a N(0,I) distribution
#psi_pdk[p,d,k] is the log-odds at node p in sample d and subcommunity k
psi_pdk = array(0,dim=c(p,D,K))
for(k in 1:K){
for(d in 1:D){
psi_pdk[,d,k] = chol(diag(p)) %*% matrix(stats::rnorm(p,0,1),nrow=p)
}
}
#convert log-odds psi_pdk into probabilities theta_kda
#theta_kda[k,d,a] is the probability in subcommunity k, sample d, and node a
theta_kda = array(0,dim=c(K,D,A))
for (k in 1:K){
for (d in 1:D){
for (a in 1:p){
theta_kda[k,d,internal_nodes[a]] = exp(psi_pdk[a,d,k])/(1+exp(psi_pdk[a,d,k]))
}
}
}
#find the multinoimal distribution on the leaves implied by the theta_kda
#beta_kdv[k,d,1:V] is the multinomial distirubtion on the V leaves in
#subcommunity k and sample d
beta_kdv = array(0,dim=c(K,D,V))
for (d in 1:D){
for (k in 1:K){
for (leaf in leaves){ #for each leaf
beta_kdv[k,d,leaf] = prod(theta_kda[k,d,leaf_success[[leaf]]])*prod((1-theta_kda[k,d,leaf_failures[[leaf]]]))
}
}
}
#initialize v_pdk, the polya-gamma auxiliary variables
# v_pdk[p,d,k] is the PG variable associated with node p in sample d and subcommunity k
v_pdk = array(0,dim=c(p,D,K))
for(k in 1:K){
for(d in 1:D){
for(a in 1:p){
if(nc_dnt[d,internal_nodes[a],k]<1){
v_pdk[a,d,k] = 0
} else {
v_pdk[a,d,k] = pgdraw::pgdraw(nc_dnt[d,internal_nodes[a],k],psi_pdk[a,d,k])
}
}
}
}
#first load markov chains
mu_chain_k_ip = model$Chain_Mu
Sigma_chain_k_ipp = model$Chain_Sigma
#posterior mu mean
post_mu_pk = matrix(0,nrow=p,ncol=K)
for(k in 1:K){
for(a in 1:p){
post_mu_pk[a,k] = mean(mu_chain_k_ip[[k]][,a])
}
}
#posterior Sigma means
post_Sigma_ppk = array(0,dim=c(p,p,K))
for(k in 1:K){
entry = Sigma_chain_k_ipp[[k]]
runtime = dim(entry)[1]
temp = matrix(0,nrow=p,ncol=p)
for(i in 1:runtime){
temp = temp + entry[i,,]
}
post_Sigma_ppk[,,k] = temp/runtime
}
post_Sigma_ppk[1,1,]
#initialize covariance matrices Sigma_ppk from the posterior means
#Sigma_ppk[,,k] is the covariance matrix associated with subcommunity k
Sigma_ppk = array(0,dim=c(p,p,K))
for(k in 1:K){
Sigma_ppk[,,k] = post_Sigma_ppk[,,k]
}
#find the inverse matrices W_ppk
#W_ppk[,,k] is the inverse of Sigma_ppk[,,k]
W_ppk = array(0,dim=c(p,p,K))
for(k in 1:K){
for(a in 1:p){
W_ppk[a,a,k] = 1/Sigma_ppk[a,a,k]
}
}
#initialize mean vectors mu_pk from the posterior means
#mu_pk[p,k] is the mean log-odds for node p in subcommunity k
mu_pk = matrix(0,nrow=p,ncol=K)
for(k in 1:K){
mu_pk[,k] = post_mu_pk[,k]
}
#Pre-allocate chains
chain_phi_dki = array(0,dim=c(D,K,iterations))
psi_chain_k_ipd = NULL
for(k in 1:K){
psi_chain_k_ipd[[k]] = array(0,dim=c(iterations,p,D))
}
2+2
Rcpp::compileAttributes()
devtools::document()
devtools::install()
library(devtools)
library(Rcpp)
library(RcppArmadillo)
library(roxygen2)
Rcpp::compileAttributes()
devtools::document()
devtools::install(build_vignettes = TRUE)
browseVignettes("LTNLDA")
library(Rcpp)
library(RcppArmadillo)
library(roxygen2)
library(devtools)
Rcpp::compileAttributes()
devtools::document()
devtools::install(build_vignettes = TRUE)
library(LTNLDA)
Rcpp::compileAttributes()
devtools::document()
devtools::install(build_vignettes = TRUE)
library(devtools)
library(Rcpp)
library(RcppArmadillo)
devtools::document()
Rcpp::compileAttributes()
Rcpp::compileAttributes()
devtools::document()
Rcpp::compileAttributes()
devtools::document()
Rcpp::compileAttributes()
devtools::document()
library(Rcpp)
library(RcppArmadillo)
library(devtools)
Rcpp::compileAttributes()
Rcpp::compileAttributes()
devtools::document()
Rcpp::compileAttributes()
devtools::document()
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
Rcpp::compileAttributes()
devtools::document()
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin)
Rcpp::compileAttributes()
devtools::document()
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin)
Rcpp::compileAttributes()
devtools::document()
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin)
sum = Summary(model)
sum
matplot(model$Chain_Phi)
model$Chain_Phi
dim(model$Chain_Phi)
matplot(model$Chain_Phi[1,,],type="l")
matplot(t(model$Chain_Phi[1,,]),type="l")
matplot(t(model$Chain_Phi[,,]),type="l")
model$Mean_Post_Phi_d
dim(model$Chain_Sigma)
length(model$Chain_Sigma)
length(model$Chain_Sigma[[1]])
dim(model$Chain_Sigma[[1]])
(model$Chain_Sigma[[1]][30,,])
hist(model$Chain_Sigma[[1]][30,,])
Rcpp::compileAttributes()
devtools::document()
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin)
sum = Summary(model)
sum
devtools::install(build_vignettes = TRUE)
######################
# Gibbs Sampler Test #
######################
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin)
sum = Summary(model)
sum
hist(model$Chain_Sigma[[1]][30,,])
length(which(model$Chain_Sigma[[1]][30,,]>0.55))
sum(which(model$Chain_Sigma[[1]][30,,]>0.55))
length(which(model$Chain_Sigma[[1]][30,,]>0.55))
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin)
sum = Summary(model)
sum
length(which(model$Chain_Sigma[[1]][30,,]>0.55))
######################
# Gibbs Sampler Test #
######################
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 300
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin, a_L = 10^4)
sum = Summary(model)
sum
length(which(model$Chain_Sigma[[1]][30,,]>0.55))
#the cov version does not have obviously larger covariances than the normal version - in fact it may have less - but seems to run into the identifiability problem
hist(model$Chain_Sigma[[1]][30,,])
######################
# Gibbs Sampler Test #
######################
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 1
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin, a_L = 10^4)
sum = Summary(model)
sum
hist(model$Chain_Sigma[[1]][30,,])
length(which(model$Chain_Sigma[[1]][30,,]>0.55))
#the cov version does not have obviously larger covariances than the normal version - in fact it may have less - but seems to run into the identifiability problem
matplot(model$Chain_Phi[1,,],type="l")
matplot(t(model$Chain_Phi[1,,]),type="l")
######################
# Gibbs Sampler Test #
######################
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 0
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin, a_L = 10^4)
sum = Summary(model)
sum
hist(model$Chain_Sigma[[1]][30,,])
length(which(model$Chain_Sigma[[1]][30,,]>0.55))
#the cov version does not have obviously larger covariances than the normal version - in fact it may have less - but seems to run into the identifiability problem
#even when set the priors so that the variance is practically zero, we still observe this phenomeona
#maybe we're starting there and not moving?
matplot(t(model$Chain_Phi[1,,]),type="l")
hist(model$Chain_Sigma)
hist(model$Chain_Sigma[[1]])
hist(model$Chain_Sigma[[2]])
model$Chain_psi
model$Chain_Psi
dim(model$Chain_Psi)
dim(model$Chain_Psi[[1]])
matplot(model$Chain_Psi[[1]][1,,],type="l")
matplot(t(model$Chain_Psi[[1]][1,,],type="l")
matplot(t(model$Chain_Psi[[1]][1,,]),type="l")
matplot(model$Mean_Post_Beta_kd,type="l")
dim(model$Mean_Post_Beta_kd)
matplot(model$Mean_Post_Beta_kd[1,,],type="l")
matplot(model$Mean_Post_Beta_kd[2,,],type="l")
matplot(dtm,type="l")
dtm(ps)
otu(ps)
otu_table()
phyloseq::otu_table()
phyloseq::otu_table(ps)
matplot(phyloseq::otu_table(ps),type="l")
matplot(t(phyloseq::otu_table(ps)),type="l")
######################
# Gibbs Sampler Test #
######################
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 0
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin, a_L = 10, a_U = 10)
sum = Summary(model)
sum
hist(model$Chain_Sigma[[1]][iterations,,])
length(which(model$Chain_Sigma[[1]][iterations,,]>0.55))
#the cov version does not have obviously larger covariances than the normal version - in fact it may have less - but seems to run into the identifiability problem
#even when set the priors so that the variance is practically zero, we still observe this phenomeona
# we do not start with everything in one subcomunity, but we go there after 1/2 iterations...not sure why
#needs further, and probably specailized, investigaiton
#it doesn't seem liek we're getting crazy cross-sample heterogeneity, though we are getting a bit
matplot(t(model$Chain_Phi[1,,]),type="l")
matplot(t(model$Chain_Psi[1,,]),type="l")
matplot(t(model$Chain_Psi[[1]][1,,]),type="l")
matplot(t(model$Chain_Psi[[1]][,1,]),type="l")
matplot(t(model$Chain_Psi[[1]][,,1]),type="l")
matplot((model$Chain_Psi[[1]][1,,]),type="l")
matplot((model$Chain_Psi[[1]][,1,]),type="l")
######################
# Gibbs Sampler Test #
######################
library(LTNLDA)
set.seed(1)
#load dataset
data("ps",package = "LTNLDA")
#Run Analsyis
K = 2
iterations = 30
burnin = 0
thin = 10
model = LTNLDA_cov(ps = ps, K = K, iterations = iterations, burnin = burnin, thin = thin, a_L = 10^4, a_U = 10^4)
sum = Summary(model)
sum
hist(model$Chain_Sigma[[1]][iterations,,])
length(which(model$Chain_Sigma[[1]][iterations,,]>0.55))
#the cov version does not have obviously larger covariances than the normal version - in fact it may have less - but seems to run into the identifiability problem
#even when set the priors so that the variance is practically zero, we still observe this phenomeona
# we do not start with everything in one subcomunity, but we go there after 1/2 iterations...not sure why
#needs further, and probably specailized, investigaiton
#it doesn't seem liek we're getting crazy cross-sample heterogeneity, though we are getting a bit
matplot(model$Chain_Phi[[1]][1,,],type="l")
matplot(model$Chain_Phi[[1]][1,],type="l")
matplot(model$Chain_Psi[[1]][1,,],type="l")
matplot(model$Chain_Psi[[1]][1,,],type="l")
matplot(model$Chain_Psi[[1]][,1,],type="l")
matplot(model$Chain_Psi[[2]][,1,],type="l")
matplot(model$Chain_Psi[[1]][,,1],type="l")
matplot(model$Chain_Psi[[1]][,,2],type="l")
matplot(model$Chain_Psi[[1]][,,2],type="l")
matplot(model$Chain_Psi[[1]][,2,],type="l")
matplot(model$Chain_Psi[[1]][,3,],type="l")
matplot(model$Chain_Psi[[1]][,4,],type="l")
matplot(model$Chain_Psi[[1]][,5,],type="l")
matplot(model$Chain_Psi[[1]][,6,],type="l")
matplot(model$Chain_Psi[[1]][,7,],type="l")
matplot(model$Chain_Psi[[1]][,8,],type="l")
